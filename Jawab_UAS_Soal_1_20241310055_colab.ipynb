{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# JAWABAN UAS DEEP LEARNING - SOAL 1\n",
                "**NAMA:** FARIS ALI HUSAMUDDIN  \n",
                "**NPM:** 20241310055\n",
                "\n",
                "**Mata Kuliah:** Deep Learning\n",
                "**Topik:** Sentiment Analysis (YouTube Comments)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install pandas numpy seaborn matplotlib scikit-learn nltk PySastrawi"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import seaborn as sns\n",
                "import matplotlib.pyplot as plt\n",
                "import nltk\n",
                "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
                "from nltk.corpus import stopwords\n",
                "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.feature_extraction.text import TfidfVectorizer\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
                "import re\n",
                "import pickle\n",
                "\n",
                "nltk.download('stopwords', quiet=True)\n",
                "nltk.download('punkt', quiet=True)\n",
                "nltk.download('punkt_tab', quiet=True)\n",
                "nltk.download('wordnet', quiet=True)\n",
                "nltk.download('omw-1.4', quiet=True)\n",
                "print(\"Library berhasil diimport!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Definisikan Fungsi Preprocessing (6 Poin Lengkap)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def npm_20241310055_preprocessing_lengkap(text):\n",
                "    # 1. Normalization (Case Folding + Regex)\n",
                "    text = str(text).lower()\n",
                "    text = re.sub(r'http\\S+', '', text)    # Hapus URL\n",
                "    text = re.sub(r'@[A-Za-z0-9]+', '', text) # Hapus Mention\n",
                "    text = re.sub(r'[^a-zA-Z\\s]', '', text) # Hapus Angka & Simbol (Parsing Pattern)\n",
                "    \n",
                "    # Setup Stopwords\n",
                "    try:\n",
                "        stopwords_id = stopwords.words('indonesian')\n",
                "    except:\n",
                "        stopwords_id = ['yang', 'dan', 'di', 'ke', 'dari', 'ini', 'itu']\n",
                "    stopwords_en = stopwords.words('english')\n",
                "    \n",
                "    # 3. Tokenization (NLTK)\n",
                "    words = nltk.word_tokenize(text)\n",
                "    \n",
                "    # 4. Stopword Removal\n",
                "    words = [w for w in words if w not in stopwords_id and w not in stopwords_en]\n",
                "    \n",
                "    # 5. Lemmatization (WordNet)\n",
                "    lemmatizer = WordNetLemmatizer()\n",
                "    words = [lemmatizer.lemmatize(w) for w in words]\n",
                "    \n",
                "    # 6. Stemming (Skipped - User Request: English Stemming destroys Indo words)\n",
                "    # stemmer = PorterStemmer()\n",
                "    # words = [stemmer.stem(w) for w in words]\n",
                "    \n",
                "    return \" \".join(words)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Proses 1: Input Data Training"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "filename = 'dataset_labeled_final.csv'\n",
                "\n",
                "try:\n",
                "    df = pd.read_csv(filename)\n",
                "    print(f\"[BERHASIL] Dataset dimuat: {len(df)} baris\")\n",
                "    print(df.head())\n",
                "    \n",
                "    # FILTER: HANYA POSITIF & NEGATIF (Binary Classification)\n",
                "    df = df[df['label'].isin(['positif', 'negatif'])]\n",
                "    print(f\"Total Data (Binary Only): {len(df)}\")\n",
                "except FileNotFoundError:\n",
                "    print(f\"[ERROR] File '{filename}' belum ada. Silakan upload dulu ke Colab!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Proses 2: Preprocessing Data\n",
                "Preprocessing Mencakup: Normalization, Tokenization, Parsing, Stopword Removal, Lemmatization, Stemming."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if 'df' in locals():\n",
                "    print(\"Sedang melakukan preprocessing lengkap (6 Poin)...\")\n",
                "    df['text'] = df['text'].astype(str)\n",
                "    df['clean_text'] = df['text'].apply(npm_20241310055_preprocessing_lengkap)\n",
                "    \n",
                "    print(\"Preprocessing Selesai.\")\n",
                "    print(df[['text', 'clean_text', 'label']].head())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Proses 3: Modeling dengan Metode Logistic Regression"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if 'df' in locals():\n",
                "    X = df['clean_text']\n",
                "    y = df['label']\n",
                "    \n",
                "    # Split Data\n",
                "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
                "    \n",
                "    # Feature Extraction (TF-IDF)\n",
                "    vectorizer = TfidfVectorizer(max_features=5000)\n",
                "    X_train_vec = vectorizer.fit_transform(X_train)\n",
                "    X_test_vec = vectorizer.transform(X_test)\n",
                "    \n",
                "    # Train Model (Logistic Regression)\n",
                "    model = LogisticRegression(max_iter=1000)\n",
                "    model.fit(X_train_vec, y_train)\n",
                "    print(\"Model Logistic Regression Berhasil Dilatih!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Proses 4: Evaluation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if 'model' in locals():\n",
                "    y_pred = model.predict(X_test_vec)\n",
                "    acc = accuracy_score(y_test, y_pred)\n",
                "    print(f\"Accuracy Score: {acc*100:.2f}%\")\n",
                "    print(\"\\nClassification Report:\")\n",
                "    print(classification_report(y_test, y_pred))\n",
                "    \n",
                "    # Confusion Matrix Visualization\n",
                "    cm = confusion_matrix(y_test, y_pred)\n",
                "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
                "    plt.title('Confusion Matrix')\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Proses 5: End Process : Prediction of Data Testing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if 'model' in locals():\n",
                "    # Save Model\n",
                "    with open('model_uas_20241310055.pkl', 'wb') as f:\n",
                "        pickle.dump(model, f)\n",
                "    with open('vectorizer_uas_20241310055.pkl', 'wb') as f:\n",
                "        pickle.dump(vectorizer, f)\n",
                "    print(\"Model Tersimpan sebagai 'model_uas_20241310055.pkl'\")\n",
                "\n",
                "    # Test Prediksi Manual\n",
                "    print(\"\\n--- HASIL PREDIKSI (PURE MODEL) ---\")\n",
                "    kalimat_1 = \"Debatnya sangat berbobot dan informatif\"\n",
                "    kalimat_2 = \"siapa pun presidennya kinerjanya sangat buruk, hancur indonesia\"\n",
                "    \n",
                "    test_cases = [kalimat_1, kalimat_2]\n",
                "    \n",
                "    for text in test_cases:\n",
                "        cleaned = npm_20241310055_preprocessing_lengkap(text)\n",
                "        \n",
                "        # Prediksi Murni dengan Model\n",
                "        vec = vectorizer.transform([cleaned])\n",
                "        final_pred = model.predict(vec)[0]\n",
                "            \n",
                "        print(f\"\\nKalimat: '{text}'\")\n",
                "        print(f\"Sentiment: {final_pred.upper()}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}